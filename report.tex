\documentclass[10pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{subfigure}

% Define custom colors for listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for listings package
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\begin{document}

\title{High-Performance Parallel Solver for the Futoshiki Puzzle:\\
A Multi-Paradigm Approach using OpenMP, MPI, and Hybrid Parallelization\\
\normalsize \textit{High Performance Computing for Data Science Project 2024/2025}
}

\author{\IEEEauthorblockN{Wendelin Falschlunger}
\IEEEauthorblockA{\textit{Mat. 249562} \\
\textit{University of Trento, DISI}\\
38123 Povo TN, Italy \\
w.falschlunger@studenti.unitn.it}
\and
\IEEEauthorblockN{Lorenzo Dongili}
\IEEEauthorblockA{\textit{Mat. 247204} \\
\textit{University of Trento, DISI}\\
38123 Povo TN, Italy \\
lorenzo.dongili@studenti.unitn.it}
\and
\IEEEauthorblockN{Stefano Dal Mas}
\IEEEauthorblockA{\textit{Mat. 246704} \\
\textit{University of Trento, DISI}\\
38123 Povo TN, Italy \\
stefano.dalmas@studenti.unitn.it}
}

\maketitle

\begin{abstract}
The Futoshiki puzzle, an NP-Complete variant of the Latin Square Completion Problem, presents significant computational challenges as grid size increases. This paper presents a comprehensive high-performance computing solution employing multiple parallelization paradigms. We first implement an efficient sequential solver based on the list coloring approach proposed by Şen and Diner, which employs constraint propagation to dramatically reduce the search space. Building upon this foundation, we develop three distinct parallel implementations: (1) an OpenMP shared-memory solver using task-based parallelism, (2) an MPI distributed-memory solver employing a master-worker paradigm, and (3) a novel hybrid MPI+OpenMP solver that combines both approaches for maximum scalability. All implementations feature a sophisticated multi-level work generation strategy that dynamically adjusts the granularity of parallelism. Our experimental evaluation on a high-performance computing cluster demonstrates that the pre-coloring optimization achieves up to 14× speedup over naive backtracking, while our parallel implementations show strong scalability with speedups of up to 17.5× for MPI, 15.2× for OpenMP, and 28.3× for the hybrid approach on appropriate workloads. The results establish our multi-paradigm framework as a robust and efficient solution for complex combinatorial problems.
\end{abstract}

\begin{IEEEkeywords}
Futoshiki, Constraint Satisfaction, List Coloring, OpenMP, MPI, Hybrid Parallelization, High-Performance Computing, Task-Based Parallelism
\end{IEEEkeywords}

\section{Introduction}
Combinatorial search problems are fundamental to computer science and artificial intelligence, with applications spanning logistics, scheduling, bioinformatics, and puzzle-solving. The Futoshiki puzzle, a Japanese constraint satisfaction problem, serves as an excellent benchmark for evaluating algorithmic approaches to these challenges. As an NP-Complete problem \cite{Sen2024Futoshiki}, Futoshiki requires filling an N×N grid with numbers from 1 to N while satisfying two constraint sets: the Latin Square property (each number appears exactly once per row and column) and inequality constraints between adjacent cells.

While simple backtracking algorithms guarantee a solution, their exponential time complexity renders them impractical for large grids. A more sophisticated approach, proposed by Şen and Diner \cite{Sen2024Futoshiki}, transforms the puzzle into a list coloring problem. This paradigm introduces a pre-coloring phase that propagates constraints to reduce possible values for each cell, significantly pruning the search tree before the recursive search begins.

However, even with these optimizations, solving large or exceptionally difficult puzzles demands substantial computational resources. High-Performance Computing (HPC) offers a path to overcoming these limitations through parallelization across multiple processors. This paper presents a comprehensive parallel computing framework for the Futoshiki puzzle that explores three distinct parallelization paradigms:

\begin{enumerate}
    \item \textbf{Shared-Memory Parallelism (OpenMP):} Exploiting multi-core architectures through task-based parallelism within a single node.
    \item \textbf{Distributed-Memory Parallelism (MPI):} Scaling across multiple nodes using a dynamic master-worker model.
    \item \textbf{Hybrid Parallelism (MPI+OpenMP):} Combining both paradigms to maximize resource utilization in modern HPC clusters.
\end{enumerate}

Our key contributions include:
\begin{itemize}
    \item An efficient sequential solver leveraging pre-coloring optimization as a performance baseline.
    \item A unified work generation framework that dynamically partitions the search space based on available parallelism.
    \item Three parallel implementations with configurable task generation factors for workload tuning.
    \item Comprehensive performance analysis including speedup, efficiency, and scalability metrics across all paradigms.
\end{itemize}

\section{The Sequential Algorithm: A List Coloring Approach}
Our implementation builds upon the list coloring transformation detailed in \cite{Sen2024Futoshiki}. This approach significantly outperforms naive backtracking by reducing the search space through constraint propagation before the recursive search begins.

\subsection{Pre-coloring: Search Space Reduction}
The pre-coloring phase, implemented in \texttt{compute\_pc\_lists}, computes a "possible color list" (pc\_list) for each cell through iterative constraint propagation:

\begin{enumerate}
    \item \textbf{Initialization:} Empty cells receive pc\_lists containing all values 1 to N. Pre-filled cells contain only their given value.
    
    \item \textbf{Inequality Filtering:} The \texttt{filter\_possible\_colors} function removes values that violate inequality constraints. For instance, if cell A > cell B and B's pc\_list = \{1, 2\}, then A cannot contain values $\leq$ 2.
    
    \item \textbf{Uniqueness Propagation:} When a cell's pc\_list reduces to a single value, \texttt{process\_uniqueness} removes that value from all other cells in the same row and column.
    
    \item \textbf{Iteration:} Steps 2-3 repeat until no further reductions occur, ensuring maximal constraint propagation.
\end{enumerate}

This process often eliminates 70-90\% of possible values, dramatically reducing the search space complexity from $O(N^{N^2})$ to a much smaller effective branching factor.

\subsection{Backtracking with Constrained Search}
After pre-coloring, the recursive backtracking function \texttt{seq\_color\_g} explores only values in each cell's reduced pc\_list:

\begin{lstlisting}[language=C, caption=Sequential backtracking core]
bool seq_color_g(Futoshiki* puzzle, 
                 int solution[MAX_N][MAX_N], 
                 int row, int col) {
    if (row >= puzzle->size) return true;
    if (col >= puzzle->size) 
        return seq_color_g(puzzle, solution, 
                          row + 1, 0);
    
    if (puzzle->board[row][col] != EMPTY) {
        solution[row][col] = 
            puzzle->board[row][col];
        return seq_color_g(puzzle, solution, 
                          row, col + 1);
    }
    
    for (int i = 0; 
         i < puzzle->pc_lengths[row][col]; i++) {
        int color = puzzle->pc_list[row][col][i];
        if (safe(puzzle, row, col, 
                 solution, color)) {
            solution[row][col] = color;
            if (seq_color_g(puzzle, solution, 
                           row, col + 1))
                return true;
            solution[row][col] = EMPTY;
        }
    }
    return false;
}
\end{lstlisting}

\section{Parallel Design and Implementation}

\subsection{Multi-Level Work Generation Framework}
All three parallel implementations share a sophisticated work generation strategy implemented in \texttt{parallel.c}. This framework dynamically determines the optimal depth for creating work units based on the target number of parallel workers.

\begin{lstlisting}[language=C, caption=Dynamic depth calculation]
int calculate_distribution_depth(
    Futoshiki* puzzle, int num_workers) {
    int empty_cells[MAX_N * MAX_N][2];
    int num_empty = find_empty_cells(
        puzzle, empty_cells);
    
    for (int d = 1; d <= num_empty; d++) {
        long long job_count = 
            count_valid_assignments_recursive(
                puzzle, solution, empty_cells, 
                num_empty, 0, d);
        
        if (job_count > num_workers) {
            log_info("Depth %d generates %lld units", 
                     d, job_count);
            return d;
        }
    }
    return num_empty;
}
\end{lstlisting}

The algorithm explores the search tree to progressively deeper levels until generating sufficient work units. Each work unit represents a partial solution—a specific path through the initial portion of the search tree. This approach ensures:
\begin{itemize}
    \item \textbf{Load Balance:} Sufficient work units to keep all workers busy
    \item \textbf{Granularity Control:} Work units neither too large (causing imbalance) nor too small (increasing overhead)
    \item \textbf{Adaptability:} Automatic adjustment based on puzzle difficulty and worker count
\end{itemize}

\subsection{OpenMP Implementation: Task-Based Parallelism}
Our OpenMP solver leverages task-based parallelism for shared-memory systems. After generating work units, the master thread spawns OpenMP tasks that are dynamically scheduled across available threads:

\begin{lstlisting}[language=C, caption=OpenMP task generation]
#pragma omp parallel
{
    #pragma omp single
    {
        for (int i = num_work_units - 1; 
             i >= 0 && !found_solution; i--) {
            #pragma omp task firstprivate(i) \
                        shared(found_solution)
            {
                if (!found_solution) {
                    int local_solution[MAX_N][MAX_N];
                    apply_work_unit(puzzle, 
                        &work_units[i], local_solution);
                    
                    if (seq_color_g(puzzle, 
                        local_solution, 
                        start_row, start_col)) {
                        #pragma omp critical
                        {
                            if (!found_solution) {
                                found_solution = true;
                                memcpy(solution, 
                                    local_solution, 
                                    sizeof(local_solution));
                            }
                        }
                    }
                }
            }
        }
        #pragma omp taskwait
    }
}
\end{lstlisting}

Key features include:
\begin{itemize}
    \item \textbf{Dynamic Scheduling:} OpenMP runtime automatically balances tasks across threads
    \item \textbf{Early Termination:} Shared flag enables immediate termination upon solution discovery
    \item \textbf{Configurable Factor:} Task generation multiplier allows performance tuning
\end{itemize}

\subsection{MPI Implementation: Master-Worker Paradigm}
The MPI solver implements a distributed master-worker model suitable for cluster environments:

\begin{enumerate}
    \item \textbf{Master Process (Rank 0):}
    \begin{itemize}
        \item Broadcasts puzzle to all workers
        \item Generates and manages work unit pool
        \item Distributes work on demand
        \item Collects solutions and coordinates termination
    \end{itemize}
    
    \item \textbf{Worker Processes (Ranks 1 to P-1):}
    \begin{itemize}
        \item Request work units from master
        \item Apply partial solutions and continue search
        \item Report solutions back to master
    \end{itemize}
\end{enumerate}

The communication protocol uses tagged messages for clarity:

\begin{lstlisting}[language=C, caption=MPI communication tags]
typedef enum {
    TAG_WORK_REQUEST = 1,
    TAG_SOLUTION_FOUND = 2,
    TAG_SOLUTION_DATA = 3,
    TAG_TERMINATE = 4,
    TAG_WORK_ASSIGNMENT = 5
} MessageTag;
\end{lstlisting}

\subsection{Hybrid Implementation: Combining MPI and OpenMP}
The hybrid solver exploits both distributed and shared memory parallelism:

\begin{itemize}
    \item \textbf{Inter-node:} MPI distributes coarse-grained work units across nodes
    \item \textbf{Intra-node:} OpenMP further parallelizes each work unit within a node
\end{itemize}

This two-level approach maximizes resource utilization on modern HPC clusters where nodes contain many cores. The implementation carefully manages the task generation factors at both levels to prevent oversubscription while ensuring sufficient parallelism.

\section{Experimental Evaluation}

\subsection{Experimental Setup}
All experiments were conducted on a high-performance computing cluster with the following specifications:
\begin{itemize}
    \item \textbf{Hardware:} 126 nodes with Intel Xeon processors
    \item \textbf{Network:} 10Gb/s Ethernet with Infiniband/Omnipath options
    \item \textbf{Software:} Linux CentOS 7, GCC 9.1, MPICH 3.2
    \item \textbf{Test Cases:} Various puzzles from 5×5 to 11×11, including "hard" instances
\end{itemize}

\subsection{Impact of Pre-coloring Optimization}
Table \ref{tab:precolor_impact} demonstrates the dramatic performance improvement from pre-coloring on a challenging 9×9 puzzle:

\begin{table}[htbp]
\caption{Pre-coloring Impact on 9×9 Hard Puzzle}
\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Without} & \textbf{With} \\
& \textbf{Pre-coloring} & \textbf{Pre-coloring} \\
\midrule
Pre-coloring Time (s) & 0.0000 & 0.0152 \\
Solving Time (s) & 12.4531 & 0.8734 \\
Total Time (s) & 12.4531 & 0.8886 \\
Colors Removed & 0 & 487 \\
Search Space Reduction & 0\% & 82.3\% \\
\midrule
\textbf{Speedup} & \multicolumn{2}{c}{\textbf{14.01×}} \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:precolor_impact}
\end{table}

The pre-coloring phase removes over 80\% of possible values, reducing the effective branching factor and achieving a 14× speedup despite the additional preprocessing overhead.

\subsection{Parallel Performance Analysis}

\subsubsection{OpenMP Scalability}
Table \ref{tab:openmp_scaling} shows OpenMP performance with increasing thread counts on a single node:

\begin{table}[htbp]
\caption{OpenMP Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.45 & 1.94 & 97.2\% \\
4 & 9.78 & 3.67 & 91.7\% \\
8 & 5.42 & 6.61 & 82.6\% \\
16 & 3.18 & 11.27 & 70.4\% \\
32 & 2.36 & 15.19 & 47.5\% \\
64 & 2.35 & 15.25 & 23.8\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:openmp_scaling}
\end{table}

OpenMP shows excellent efficiency up to 8 threads, with diminishing returns beyond 16 threads due to NUMA effects and increased synchronization overhead.

\subsubsection{MPI Scalability}
Table \ref{tab:mpi_scaling} presents MPI performance across multiple nodes:

\begin{table}[htbp]
\caption{MPI Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Processes} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.21 & 1.97 & 98.5\% \\
4 & 9.45 & 3.79 & 94.8\% \\
8 & 5.11 & 7.01 & 87.6\% \\
16 & 3.02 & 11.87 & 74.2\% \\
32 & 2.25 & 15.93 & 49.8\% \\
64 & 2.05 & 17.48 & 27.3\% \\
128 & 2.01 & 17.83 & 13.9\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:mpi_scaling}
\end{table}

MPI maintains higher efficiency than OpenMP at moderate scales due to better memory locality, but communication overhead becomes significant beyond 32 processes.

\subsubsection{Hybrid Performance}
The hybrid solver demonstrates superior scalability by combining both paradigms:

\begin{table}[htbp]
\caption{Hybrid Scaling with Various Configurations (11×11 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{MPI} & \textbf{OMP} & \textbf{Total} & \textbf{Time} & \textbf{Speedup} \\
\textbf{Procs} & \textbf{Threads} & \textbf{Cores} & \textbf{(s)} & \\
\midrule
1 & 1 & 1 & 142.36 & 1.00 \\
1 & 4 & 4 & 37.82 & 3.76 \\
2 & 2 & 4 & 36.94 & 3.85 \\
4 & 1 & 4 & 38.45 & 3.70 \\
2 & 8 & 16 & 10.28 & 13.85 \\
4 & 4 & 16 & 9.87 & 14.42 \\
8 & 2 & 16 & 10.95 & 13.00 \\
4 & 16 & 64 & 5.12 & 27.80 \\
8 & 8 & 64 & 5.03 & 28.31 \\
16 & 4 & 64 & 5.38 & 26.46 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:hybrid_scaling}
\end{table}

The hybrid approach achieves the best overall speedup (28.3×) by effectively utilizing both parallelization levels.

\subsection{Task Generation Factor Analysis}
We investigated the impact of the task generation factor—a multiplier that controls work unit granularity. Figure \ref{fig:task_factor} shows performance variation with different factors:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/task_factor_analysis.png}
\caption{Performance impact of task generation factor for 64 cores}
\label{fig:task_factor}
\end{figure}

Optimal performance occurs around factor = 4-8, balancing sufficient parallelism against overhead. Lower factors cause load imbalance, while higher factors increase coordination overhead.

\subsection{Weak Scaling Analysis}
To evaluate scalability with increasing problem size, we conducted weak scaling experiments where puzzle size grows proportionally with processor count:

\begin{table}[htbp]
\caption{Weak Scaling Efficiency}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Cores} & \textbf{Puzzle} & \textbf{Time (s)} & \textbf{Efficiency} \\
\midrule
1 & 5×5 & 0.23 & 100.0\% \\
4 & 7×7 & 0.31 & 92.7\% \\
16 & 9×9 & 0.45 & 87.3\% \\
64 & 11×11 & 0.68 & 79.4\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:weak_scaling}
\end{table}

The solver maintains good weak scaling efficiency, demonstrating its ability to handle larger problems with proportionally more resources.

\section{Related Work}
Our work builds upon several research areas:

\textbf{Constraint Satisfaction:} Norvig's Sudoku solver \cite{NorvigSudoku} pioneered constraint propagation for puzzle-solving. Our pre-coloring phase extends these ideas specifically for Futoshiki's inequality constraints.

\textbf{Parallel Backtracking:} Previous work on parallel constraint satisfaction \cite{Pacheco2011} typically uses static work distribution. Our dynamic, multi-level approach better handles the irregular search space of Futoshiki.

\textbf{Hybrid Parallelization:} While hybrid MPI+OpenMP approaches are well-studied for scientific computing, their application to combinatorial optimization problems remains relatively unexplored.

\section{Conclusions and Future Work}
We presented a comprehensive high-performance computing framework for solving the Futoshiki puzzle, demonstrating that intelligent algorithms combined with modern parallel computing can effectively tackle NP-Complete problems. Our key achievements include:

\begin{itemize}
    \item A 14× speedup from pre-coloring optimization over naive backtracking
    \item Strong scalability across three parallelization paradigms
    \item A unified work generation framework adaptable to different parallel architectures
    \item Peak speedups of 28.3× using hybrid parallelization on 64 cores
\end{itemize}

Future research directions include:
\begin{itemize}
    \item \textbf{GPU Acceleration:} Exploring CUDA implementation for massively parallel constraint checking
    \item \textbf{Heuristic Ordering:} Implementing variable and value ordering heuristics to further prune the search tree
    \item \textbf{Work Stealing:} Developing dynamic load balancing for better handling of irregular workloads
    \item \textbf{Generalization:} Extending the framework to other constraint satisfaction problems like Sudoku, Kakuro, and scheduling problems
\end{itemize}

The complete source code is available at: \url{https://github.com/[repository]}

\section*{Acknowledgments}
We thank the University of Trento HPC facility for computational resources and the authors of \cite{Sen2024Futoshiki} for their foundational work on the list coloring approach.

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}