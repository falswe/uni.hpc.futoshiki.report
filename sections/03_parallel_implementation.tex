\section{Parallel Design and Implementation}

\subsection{Multi-Level Work Generation Framework}
All three parallel implementations share a sophisticated work generation strategy implemented in \texttt{parallel.c}. This framework dynamically determines the optimal depth for creating work units based on the target number of parallel workers.

\begin{lstlisting}[language=C, caption=Dynamic depth calculation]
int calculate_distribution_depth(
    Futoshiki* puzzle, int num_workers) {
    int empty_cells[MAX_N * MAX_N][2];
    int num_empty = find_empty_cells(
        puzzle, empty_cells);
    
    for (int d = 1; d <= num_empty; d++) {
        long long job_count = 
            count_valid_assignments_recursive(
                puzzle, solution, empty_cells, 
                num_empty, 0, d);
        
        if (job_count > num_workers) {
            log_info("Depth %d generates %lld units", 
                     d, job_count);
            return d;
        }
    }
    return num_empty;
}
\end{lstlisting}

The algorithm explores the search tree to progressively deeper levels until generating sufficient work units. Each work unit represents a partial solution—a specific path through the initial portion of the search tree. This approach ensures:
\begin{itemize}
    \item \textbf{Load Balance:} Sufficient work units to keep all workers busy
    \item \textbf{Granularity Control:} Work units neither too large (causing imbalance) nor too small (increasing overhead)
    \item \textbf{Adaptability:} Automatic adjustment based on puzzle difficulty and worker count
\end{itemize}

\subsection{OpenMP Implementation: Task-Based Parallelism}
Our OpenMP solver leverages task-based parallelism for shared-memory systems. After generating work units, the master thread spawns OpenMP tasks that are dynamically scheduled across available threads:

\begin{lstlisting}[language=C, caption=OpenMP task generation]
#pragma omp parallel
{
    #pragma omp single
    {
        for (int i = num_work_units - 1; 
             i >= 0 && !found_solution; i--) {
            #pragma omp task firstprivate(i) \
                        shared(found_solution)
            {
                if (!found_solution) {
                    int local_solution[MAX_N][MAX_N];
                    apply_work_unit(puzzle, 
                        &work_units[i], local_solution);
                    
                    if (seq_color_g(puzzle, 
                        local_solution, 
                        start_row, start_col)) {
                        #pragma omp critical
                        {
                            if (!found_solution) {
                                found_solution = true;
                                memcpy(solution, 
                                    local_solution, 
                                    sizeof(local_solution));
                            }
                        }
                    }
                }
            }
        }
        #pragma omp taskwait
    }
}
\end{lstlisting}

Key features include:
\begin{itemize}
    \item \textbf{Dynamic Scheduling:} OpenMP runtime automatically balances tasks across threads
    \item \textbf{Early Termination:} Shared flag enables immediate termination upon solution discovery
    \item \textbf{Configurable Factor:} Task generation multiplier allows performance tuning
\end{itemize}

\subsection{MPI Implementation: Master-Worker Paradigm}
The MPI solver implements a distributed master-worker model suitable for cluster environments:

\begin{enumerate}
    \item \textbf{Master Process (Rank 0):}
    \begin{itemize}
        \item Broadcasts puzzle to all workers
        \item Generates and manages work unit pool
        \item Distributes work on demand
        \item Collects solutions and coordinates termination
    \end{itemize}
    
    \item \textbf{Worker Processes (Ranks 1 to P-1):}
    \begin{itemize}
        \item Request work units from master
        \item Apply partial solutions and continue search
        \item Report solutions back to master
    \end{itemize}
\end{enumerate}

The communication protocol uses tagged messages for clarity:

\begin{lstlisting}[language=C, caption=MPI communication tags]
typedef enum {
    TAG_WORK_REQUEST = 1,
    TAG_SOLUTION_FOUND = 2,
    TAG_SOLUTION_DATA = 3,
    TAG_TERMINATE = 4,
    TAG_WORK_ASSIGNMENT = 5
} MessageTag;
\end{lstlisting}

This design enables dynamic load balancing—workers request work only when ready, automatically handling heterogeneous performance and variable work unit difficulty.

\subsection{Hybrid Implementation: Combining MPI and OpenMP}
The hybrid solver exploits both distributed and shared memory parallelism:

\begin{itemize}
    \item \textbf{Inter-node:} MPI distributes coarse-grained work units across nodes
    \item \textbf{Intra-node:} OpenMP further parallelizes each work unit within a node
\end{itemize}

\begin{lstlisting}[language=C, caption=Hybrid worker with nested parallelism]
static void hybrid_worker(Futoshiki* puzzle) {
    WorkUnit work_unit;
    MPI_Status status;
    
    while (true) {
        // Request work from master via MPI
        MPI_Send(&request, 1, MPI_INT, 0, 
                 TAG_WORK_REQUEST, MPI_COMM_WORLD);
        MPI_Recv(&work_unit, sizeof(WorkUnit), 
                 MPI_BYTE, 0, MPI_ANY_TAG, 
                 MPI_COMM_WORLD, &status);
        
        if (status.MPI_TAG == TAG_TERMINATE) break;
        
        // Apply work unit and solve with OpenMP
        Futoshiki sub_puzzle;
        memcpy(&sub_puzzle, puzzle, sizeof(Futoshiki));
        apply_work_unit(&sub_puzzle, &work_unit, 
                       sub_puzzle.board);
        
        if (omp_solve(&sub_puzzle, local_solution)) {
            // Report solution via MPI
            MPI_Send(&found_flag, 1, MPI_INT, 0, 
                    TAG_SOLUTION_FOUND, MPI_COMM_WORLD);
            MPI_Send(local_solution, MAX_N * MAX_N, 
                    MPI_INT, 0, TAG_SOLUTION_DATA, 
                    MPI_COMM_WORLD);
            break;
        }
    }
}
\end{lstlisting}

This two-level approach maximizes resource utilization on modern HPC clusters where nodes contain many cores. The implementation carefully manages the task generation factors at both levels to prevent oversubscription while ensuring sufficient parallelism.