\section{Solution Proposed}
\label{sec:solution}

We now present our findings and solutions in solving Futoshiki puzzle. We start in \Cref{subsec:paper_implementation} by first understanding how the sequential algorithm proposed by Şen and Diner in \cite{Sen2024Futoshiki} work. In this section we dive into how the precoloring works in \Cref{subsubsec:precoloring} and we wrap up by understanding how the backtrack with CSP helps us solve the problem in \Cref{subsubsec:backtrack_with_csp}

After having obtained what is going to become our baseline in the validation, we move towards the parallel implementations in \Cref{subsec:parallel_implementation}: we start off by understanding how the dynamic load balancing works in \Cref{subsubsec:dynamic_load_balancing}, as it is a sophisticated algorithm at the base of every parallel implementation. We then move towards the MPI implementation in \cref{subsubsec:mpi_implementation}, then to the OpenMP implementation in \cref{subsubsec:omp_implementation} and finally to the hybrid implementation in \cref{subsubsec:hybrid_implementation}.

\subsection{Implementing The Sequential Algorithm}
\label{subsec:paper_implementation}
Our implementation builds upon the list coloring transformation detailed by Şen and Diner in \cite{Sen2024Futoshiki}. This approach significantly outperforms naive backtracking by reducing the search space through constraint propagation before the recursive search begins. 

\subsubsection{Pre-coloring: Search Space Reduction}
\label{subsubsec:precoloring}
The pre-coloring phase, implemented in \texttt{compute\_pc\_lists}, computes a "possible color list" (pc\_list) for each cell through iterative constraint propagation:

\begin{enumerate}
    \item \textbf{Initialization:} Empty cells receive pc\_lists containing all values 1 to N. Pre-filled cells contain only their given value.
    
    \item \textbf{Inequality Filtering:} The \texttt{filter\_possible\_colors} function removes values that violate inequality constraints. For instance, if cell A $\gt$ cell B and A's pc\_list = \{1, 2\}, then B cannot contain values $\lt$ 2.
    
    \item \textbf{Uniqueness Propagation:} When a cell's pc\_list reduces to a single value, \texttt{process\_uniqueness} removes that value from all other cells in the same row and column. To continue with our example, as B can have value 1 and 2, and as A can have $n \lt 2$, it would mean that A is going to hold 1 as it is the only integer strictly lower than 2, and B therefore would hold 2, as its pc\_list had two values and we need to respect the \textit{Latin Square} property.
    
    \item \textbf{Iteration:} Steps 2-3 repeat until no further reductions occur, ensuring maximal constraint propagation.
\end{enumerate}

This process often eliminates 70-90\% of possible values, dramatically reducing the search space complexity from $O(N^{N^2})$ to a much smaller effective branching factor.

\subsubsection{Backtracking with Constrained Search}
\label{subsubsec:backtrack_with_csp}
After pre-coloring, the recursive backtracking function \texttt{seq\_color\_g} explores only values in each cell's reduced pc\_list:

\begin{lstlisting}[language=C, caption=Sequential backtracking core, label={listing:precoloring}]
bool seq_color_g(Futoshiki* puzzle, 
                 int solution[MAX_N][MAX_N], 
                 int row, int col) {
    if (row >= puzzle->size) return true;
    if (col >= puzzle->size) 
        return seq_color_g(puzzle, solution, 
                          row + 1, 0);
    
    if (puzzle->board[row][col] != EMPTY) {
        solution[row][col] = 
            puzzle->board[row][col];
        return seq_color_g(puzzle, solution, 
                          row, col + 1);
    }
    
    for (int i = 0; 
         i < puzzle->pc_lengths[row][col]; i++) {
        int color = puzzle->pc_list[row][col][i];
        if (safe(puzzle, row, col, 
                 solution, color)) {
            solution[row][col] = color;
            if (seq_color_g(puzzle, solution, 
                           row, col + 1))
                return true;
            solution[row][col] = EMPTY;
        }
    }
    return false;
}
\end{lstlisting}

\Cref{listing:precoloring} shows how the search space reduction is employed in the pre-coloring phase. The \texttt{safe} function validates that a color assignment doesn't violate Latin Square constraints or inequality relationships with already-placed neighbors. This combination of pre-coloring and constrained backtracking forms our efficient sequential baseline with which we evaluated the parallel solution w.r.t.

It is also important to present this approach here, as it is effectively the starting point of our solution and also parallel implementation rely on the same concepts or algorithms.

It is worth mentioning that, as Futoshiki puzzle has 1 unique solution per instance, it is not possible that two threads ever have two possible ordering of numbers such that they both lead to a solution. This means that the algorithm effectively ends once the one and only solution is found. This is needed in order to state the correctness of our approach, and the correctness of our algorithm is also based on this statement.

\subsection{Parallel Design and Implementation}
\label{subsec:parallel_implementation}
We now move towards the presentation of our parallel solutions. We start off with an explanation on why our solution differs and improves upon the state of the art by employing a dynamic load balancing algorithm, as this one is used across several parallel implementations. We then move towards MPI, OpenMP and hybrid approach. 

The aforementioned algorithm to perform the pruning of the backtrack search space via list coloring runs in polynomial time. From a practical point of view, this means that, by spending "a little bit amount of time" in the precoloring phase, the speedup that we obtain is quite high. Now, as we know from Ahmdal's Law, given a specific "Enhancement" over the existing algorithm, we can represent the speedup over a machine which is running a process as the ratio among the execution time without said Enhancement over the execution time with it being applied. This means that, as the pre-coloring phase is quite fast already and that already is a good "Enhancement", and as the actual solving of the problem is non polynomial, it makes quite a lot of sense to shift our focus towards trying to obtain some Enhancement also for the non polynomial part of the algorithm, given the fact that making the precoloring algorithm does not take too much time. Furthermore, by the law of diminishing returns, it would not make too much sense to apply other enhancement and speedup on the precoloring, as that would not lead to another high increment over the execution time of the total solution.

\subsubsection{Multi-Level Work Generation Framework}
\label{subsubsec:dynamic_load_balancing}
All three parallel implementations share a sophisticated work generation strategy implemented in \texttt{parallel.c}. This framework dynamically determines the optimal depth for creating work units based on the target number of parallel workers.

\begin{lstlisting}[language=C, caption=Dynamic depth calculation,label={listing:calculate_distribution_depth}]
int calculate_distribution_depth(
    Futoshiki* puzzle, int num_workers) {
    int empty_cells[MAX_N * MAX_N][2];
    int num_empty = find_empty_cells(
        puzzle, empty_cells);
    
    for (int d = 1; d <= num_empty; d++) {
        long long job_count = 
            count_valid_assignments_recursive(
                puzzle, solution, empty_cells, 
                num_empty, 0, d);
        
        if (job_count > num_workers) {
            log_info("Depth %d generates %lld units", 
                     d, job_count);
            return d;
        }
    }
    return num_empty;
}
\end{lstlisting}

As we can see in \Cref{listing:calculate_distribution_depth}, the algorithm explores the search tree to progressively deeper levels until generating sufficient work units. Each work unit represents a partial solution, which means that it also represent a specific path through the initial portion of the search tree. This approach ensures:
\begin{itemize}
    \item \textbf{Load Balance:} Sufficient work units to minimize the idle time in each worker.
    \item \textbf{Granularity Control:} Work units neither too large (causing imbalance) nor too small (increasing overhead).
    \item \textbf{Adaptability:} Automatic adjustment based on puzzle difficulty and worker count.
\end{itemize}

\subsubsection{MPI Implementation: Master-Worker Paradigm}
\label{subsubsec:mpi_implementation}
Here are the most interesting aspects about our MPI solution. From an high level point of view, given P as the number of Processes which are defined, the MPI solver implements a distributed master-worker model suitable for cluster environments:

\begin{enumerate}
    \item \textbf{Master Process (Rank 0):}
    \begin{itemize}
        \item Performs precoloring and fills out a basic version of the puzzle.
        \item Based on the \textit{configuration factor} given, decides the ratio of jobs to CPUs(puzzles to be solved) by evaluating the depth of the backtracking approach and opens the port to listen to the slaves.
        \item Sends 1 job to each worker via the \textit{TAG\_WORK\_ASSIGNMENT}.
        \item If a worker does not solve the job, he receives the message and sends a new job via the same tag as the previous point. If the solution is found, it broadcasts to every other worker to stop as the solution is found via a broadcast of the \textit{TAG\_TERMINATE}. Again, remember that due to the problem statement, we know that given 1 Futoshiki puzzle there exists 1 and only 1 solution. This serves as a validation to the correctness of our approach.
        \item Collects the solution found and sends it to the user.
    \end{itemize}
    
    \item \textbf{Worker Processes (Ranks 1 to P-1):}
    \begin{itemize}
        \item Polls master by asking for a job to solve via the \textit{TAG\_WORK\_REQUEST}.
        \item Tries to solve the pre colored puzzle given. If it manages, it sends a \textit{TAG\_SOLUTION\_FOUND} followed by the \textit{TAG\_SOLUTION\_DATA}, else it sends again a \textit{TAG\_WORK\_REQUEST}.
        \item Upon receiving from the master the \textit{TAG\_TERMINATE}, as this means that a solution was found, it gracefully shuts down.
    \end{itemize}
\end{enumerate}

\sd{here is a sequence diagram to better explain our approach to the reader.}
% Worker[i]                       Master (Rank 0)
%    |                                 |
%    |------ TAG\_WORK\_REQUEST --------->|  Worker asks for a job
%    |                                 |
%    |<----- TAG\_WORK\_ASSIGNMENT ------|  Master sends work unit
%    |                                 |
%    |--compute work (precolor + backtracking)-->|
%    |                                 |
%    |--- TAG\_SOLUTION\_FOUND ---------->|  Worker found solution (if success)
%    |--- TAG\_SOLUTION\_DATA ----------->|  Worker sends actual solution
%    |                                 |
%    |<----- TAG\_TERMINATE ------------|  Master tells worker to stop
%    |                                 |
%    |----- Worker exits --------------|

A careful reader might have already recognized that there is a corner case for which this approach is not feasible. It is the case if we only have 1 CPU, as being this a master-worker approach, we cannot have a master to tell no one what to do, or a worker which does not know what to do. Therefore, to avoid our single thread to be "overly specialized" -- as of either manages or works, in the case in which we have only 1 CPU available we fallback to the sequential implementation.

For clarity, we are also presenting in \Cref{listing:message_tags} the tags which our communication protocol leverages in order to achieve the aforementioned flow.

\begin{lstlisting}[language=C, caption=MPI communication tags, label={listing:message_tags}]
typedef enum {
    TAG_WORK_REQUEST = 1,
    TAG_SOLUTION_FOUND = 2,
    TAG_SOLUTION_DATA = 3,
    TAG_TERMINATE = 4,
    TAG_WORK_ASSIGNMENT = 5
} MessageTag;
\end{lstlisting}
This design enables dynamic load balancing—workers request work only when ready, automatically handling heterogeneous performance and variable work unit difficulty.


\subsubsection{OpenMP Implementation: Task-Based Parallelism}
\label{subsubsec:omp_implementation}
We can now move towards our OpenMP solver leverages task-based parallelism for shared-memory systems. After generating work units, the master thread spawns OpenMP tasks that are dynamically scheduled across available threads. In \Cref{listing:omp_algorithm} we present the most important snippet of code, which is the one used by the threads/workers to find the solution.

From an high level point of view, once the precoloring phase is performed and constant propagation, it means that the master thread can give each worker the puzzle with a partial solution, and once one finds one, sets the flag \textit{found\_solution} to true, effectively stopping every other thread as a solution is found. 

\wen{omp gives inconsistent results when running on one thread only - can be double the time like for 11x11h5 or a fraction like for 9x9h1. let me investigate this.}

\begin{lstlisting}[language=C, caption=OpenMP task generation, label={listing:omp_algorithm}]
#pragma omp parallel
{
    #pragma omp single
    {
        for (int i = num_work_units - 1; 
             i >= 0 && !found_solution; i--) {
            #pragma omp task firstprivate(i) \
                        shared(found_solution)
            {
                if (!found_solution) {
                    int local_solution[MAX_N][MAX_N];
                    apply_work_unit(puzzle, 
                        &work_units[i], local_solution);
                    
                    if (seq_color_g(puzzle, 
                        local_solution, 
                        start_row, start_col)) {
                        #pragma omp critical
                        {
                            if (!found_solution) {
                                found_solution = true;
                                memcpy(solution, 
                                    local_solution, 
                                    sizeof(local_solution));
                            }
                        }
                    }
                }
            }
        }
        #pragma omp taskwait
    }
}
\end{lstlisting}

Key features include:
\begin{itemize}
    \item \textbf{Dynamic Scheduling:} OpenMP runtime automatically balances tasks across threads.
    \item \textbf{Early Termination:} Due to the fact that we have the shared variable \textit{solution\_found}, this means that it is not required that every single thread explores all possible solutions. This leads to several fold improvements as probabilistically speaking it is quite unlikely for the solution to be found at the N-th puzzle solved.
    \item \textbf{Configurable Factor:} We have a dynamic task generation \textit{factor} which allows to assess the ratio between available threads and jobs to be created, so that one could determine the amount of "stress" to put to the system.
\end{itemize}


\sd{this is the optimization for 1 thread}
Here the careful reader might be surprised to find that also here we are leveraging the sequential solution if we only have 1 thread running. Given the proposed architecture, it is capable of running with 1 thread only, then why is this done? The reason is twofold:
\begin{itemize}
    \item In order to make our future comparison with MPI as interesting as possible, we have decided to keep the two designs as close as possible from an interface point of view.
    \item Not having overhead given by code expansion and extra metadata being inserted during compile time, as we would not use that extra capabilities with just 1 process running, we decided to rely on the more efficient sequential for this corner case.
\end{itemize}

Of course, this solution does not rely on message passing and therefore it does not require to define any additional structure, but rather by relying on the macros defined we are able to decide which variables are going to be shared or not, therefore \textit{early termination} still occurs also in this solution but leveraging the shared variable instead of the messages. 

Here is our dataflow analysis for our proposed solution:
\sd{put dataflow analysis here}

\subsubsection{Hybrid Implementation: Combining MPI and OpenMP}
\label{subsubsec:hybrid_implementation}

We are now going to present out hybrid solution, which aims at exploiting both the distributed message passing provided by MPI and combining it with the shared memory parallelism offered by OMP. In order to make the comparison among versions as similar as possible, we have thought of a two layer approach:
\begin{itemize}
    \item \textbf{Outer Layer:} This is the Inter-node part of our solution and relies on MPI, which distributes coarse-grained work units across nodes. The idea is that by abstracting away the two approaches while keeping the API commons w.r.t to the MPI and OMP solution, this part effectively behaves just like the MPI counter variant.
    \item \textbf{Inner Layer:} This is the Intra-node part of our solution and of course relies on OpenMP, which further parallelizes each work unit within a node. The idea is that the MPI serves as a wrapper over the message passing, coordination and generation of tasks, whereas the inner OMP serves to optimize the solving time of each node.
\end{itemize}


By combining this approach by having common APIs, we are able to let MPI behave as if the nodes are normal ones, and we make possible for OMP to work in parallel. Of coruse, as MPI is used to orchestrate the whole job decision and message passing, once a solution is found by a worker, we have to rely on the message passing strategy provided by MPI, and again this is done so that the comparison in performance w.r.t. the other two solution would be as seamless as possible.

\begin{lstlisting}[language=C, caption=Hybrid worker with nested parallelism, label={listing:hybrid_worker}]
static void hybrid_worker(Futoshiki* puzzle) {
    WorkUnit work_unit;
    MPI_Status status;
    
    while (true) {
        // Request work from master via MPI
        MPI_Send(&request, 1, MPI_INT, 0, 
                 TAG_WORK_REQUEST, MPI_COMM_WORLD);
        MPI_Recv(&work_unit, sizeof(WorkUnit), 
                 MPI_BYTE, 0, MPI_ANY_TAG, 
                 MPI_COMM_WORLD, &status);
        
        if (status.MPI_TAG == TAG_TERMINATE) break;
        
        // Apply work unit and solve with OpenMP
        Futoshiki sub_puzzle;
        memcpy(&sub_puzzle, puzzle, sizeof(Futoshiki));
        apply_work_unit(&sub_puzzle, &work_unit, 
                       sub_puzzle.board);
        
        if (omp_solve(&sub_puzzle, local_solution)) {
            // Report solution via MPI
            MPI_Send(&found_flag, 1, MPI_INT, 0, 
                    TAG_SOLUTION_FOUND, MPI_COMM_WORLD);
            MPI_Send(local_solution, MAX_N * MAX_N, 
                    MPI_INT, 0, TAG_SOLUTION_DATA, 
                    MPI_COMM_WORLD);
            break;
        }
    }
}
\end{lstlisting}

In \Cref{listing:hybrid_worker} we present the core of the solution, which is the code of the hybrid worker. Due to the design choices made, we can see how we are effectively relying on the APIs provided from MPI and OMP, therefore reusing already existing code without having to write already defined solving logic.