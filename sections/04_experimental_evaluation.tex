\section{Experimental Evaluation}
We conducted a series of experiments to evaluate the performance of our solver. The primary goals were to:
\begin{enumerate}
    \item Quantify the performance gain from the pre-coloring optimization.
    \item Measure the speedup and efficiency of the MPI-based parallel solver.
\end{enumerate}

\subsection{Hardware and Experimental Setup}
All experiments were run on a high-performance computing cluster with the following specifications:
\begin{itemize}
    \item \textbf{Operating System:} Linux CentOS7
    \item \textbf{Nodes:} 126 nodes, interconnected via 10Gb/s network with high-speed options (Infiniband/Omnipath).
    \item \textbf{CPU:} Intel Xeon processors.
    \item \textbf{Compiler/MPI:} GCC with MPICH 3.2.
\end{itemize}
Tests were performed on a variety of puzzles, including the provided \texttt{9x9\_hard\_3.txt}, and other puzzles of varying difficulty.

\subsection{Impact of Pre-coloring}
To isolate the benefit of the pre-coloring optimization, we ran the sequential solver on the same puzzle with and without this feature enabled. The results, summarized in Table \ref{tab:precolor_impact}, show a dramatic improvement.

\begin{table}[htbp]
\caption{Performance Impact of Pre-coloring on a 9x9 Puzzle}
\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Without Pre-coloring} & \textbf{With Pre-coloring} \\
\midrule
Pre-coloring Time (s) & 0.0000 & 0.0152 \\
Solving Time (s)      & 12.4531 & 0.8734 \\
\textbf{Total Time (s)} & \textbf{12.4531} & \textbf{0.8886} \\
\midrule
\textbf{Overall Speedup} & \multicolumn{2}{c}{\textbf{14.01x}} \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:precolor_impact}
\end{table}

Although pre-coloring introduces a small overhead, it reduces the main solving time by over an order of magnitude. This confirms that constraint propagation is an essential first step for efficiently solving Futoshiki. The performance gain is visualized in Figure \ref{fig:precoloring_impact}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/precoloring_impact.png}
\caption{Total time comparison with and without pre-coloring, highlighting the dramatic reduction in the solving phase.}
\label{fig:precoloring_impact}
\end{figure}

\subsection{Parallel Performance: Speedup and Efficiency}
We evaluated the scalability of the MPI solver by running it on a difficult puzzle with an increasing number of processes (from 1 to 64). The execution times are recorded in Table \ref{tab:mpi_times}.

\begin{table}[htbp]
\caption{Execution Times and Performance Metrics for MPI Solver}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Processes} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} \\
\midrule
1 (Sequential) & 35.84 & 1.00 & 100.0 \\
2              & 18.21 & 1.97 & 98.5 \\
4              & 9.45  & 3.79 & 94.8 \\
8              & 5.11  & 7.01 & 87.6 \\
16             & 3.02  & 11.87& 74.2 \\
32             & 2.25  & 15.93& 49.8 \\
64             & 2.05  & 17.48& 27.3 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:mpi_times}
\end{table}

\textbf{Speedup,} defined as $S_p = T_1 / T_p$, measures the performance gain from parallelization. As shown in Figure \ref{fig:speedup_chart}, the speedup increases significantly with more processes but begins to plateau around 16-32 processes. This is expected behavior, governed by Amdahl's law, where the serial portions of the code (like work generation by the master) and communication overhead begin to dominate.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/speedup_chart.png}
\caption{Speedup curve for the MPI solver. Ideal speedup is linear.}
\label{fig:speedup_chart}
\end{figure}

\textbf{Efficiency,} defined as $E_p = S_p / p$, measures how effectively the processes are utilized. Figure \ref{fig:efficiency_chart} shows that efficiency is high for a small number of processes but declines as more are added. This drop is primarily due to communication overhead and potential load imbalance, as some work units may be harder to solve than others.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/efficiency_chart.png}
\caption{Efficiency curve for the MPI solver. Efficiency drops as communication overhead increases relative to computation.}
\label{fig:efficiency_chart}
\end{figure}