\section{Experimental Evaluation}
\label{sec:evaluation}

In this section we are going to present the results gotten from our experimental evaluation.

\sd{quando hai definito i paragrafi linkali}


\subsection{Can we evaluate difficulty of a Futoshiki before solving it?}
It is important to stop for a moment and assess a crucial factor: as we have already explained in \Cref{subsec:backtrack} how Futoshiki has dynamic placed constraints, which in turn implies that the assumption we can make on the input are less strict w.r.t. Sudoku. This means that by receiving as input only the number of constraints, without knowing whey they are placed and how they are intertwined one to another, it is extremely hard to define the actual "difficulty" of the problem. It is easy to see that with a simple manipulation of the constraints presented in \Cref{fig:futoshiki_example}, one could not have easily understood the chain between values 1-4. Another factor to take into account is where the initial numbers are placed and how many we have. It is true that the more that we have, the more variables we can fix and therefore the less big the search spaces becomes, but if those constraints are placed in a "bad way", one could not find the solution easily.

\sd{do we have evidence to support this?}
We can also say that the "size matters" in this case it not strictly true. One could imagine a puzzle of a specific size and one of a slightly bigger size: let's say 5x5 and 6x6. Given those as inputs, as the correlation among constraint is so important that just like for the amount of numbers placed, they might not lead to an "easier" or "harder" puzzle by default.

\subsection{Experimental Setup}
To wrap up this consideration, we can safely say that if we are only given number of constraints, number of initialized cells and size of the problem it is not possible to give a precise estimate over a metric to evaluate the "difficulty" of the problem.
All experiments were conducted on a high-performance computing cluster with the following specifications:
\begin{itemize}
    \item \textbf{Hardware:} 126 nodes with Intel Xeon processors
    \item \textbf{Network:} 10Gb/s Ethernet with Infiniband/Omnipath options
    \item \textbf{Software:} Linux CentOS 7, GCC 9.1, MPICH 3.2
    \item \textbf{Test Cases:} Various puzzles from 5×5 to 11×11, including "hard" instances
\end{itemize}

\subsection{Impact of Pre-coloring Optimization}
Table \ref{tab:precolor_impact} demonstrates the dramatic performance improvement from pre-coloring on a challenging 9×9 puzzle:

\begin{table}[htbp]
\caption{Pre-coloring Impact on 9×9 Hard Puzzle}
\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Without} & \textbf{With} \\
& \textbf{Pre-coloring} & \textbf{Pre-coloring} \\
\midrule
Pre-coloring Time (s) & 0.0000 & 0.0152 \\
Solving Time (s) & 12.4531 & 0.8734 \\
Total Time (s) & 12.4531 & 0.8886 \\
Colors Removed & 0 & 487 \\
Search Space Reduction & 0\% & 82.3\% \\
\midrule
\textbf{Speedup} & \multicolumn{2}{c}{\textbf{14.01×}} \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:precolor_impact}
\end{table}

The pre-coloring phase removes over 80\% of possible values, reducing the effective branching factor and achieving a 14× speedup despite the additional preprocessing overhead.

\subsection{Parallel Performance Analysis}

\subsubsection{OpenMP Scalability}
Table \ref{tab:openmp_scaling} shows OpenMP performance with increasing thread counts on a single node:

\begin{table}[htbp]
\caption{OpenMP Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.45 & 1.94 & 97.2\% \\
4 & 9.78 & 3.67 & 91.7\% \\
8 & 5.42 & 6.61 & 82.6\% \\
16 & 3.18 & 11.27 & 70.4\% \\
32 & 2.36 & 15.19 & 47.5\% \\
64 & 2.35 & 15.25 & 23.8\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:openmp_scaling}
\end{table}

OpenMP shows excellent efficiency up to 8 threads, with diminishing returns beyond 16 threads due to NUMA effects and increased synchronization overhead.

\subsubsection{MPI Scalability}
Table \ref{tab:mpi_scaling} presents MPI performance across multiple nodes:

\begin{table}[htbp]
\caption{MPI Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Processes} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.21 & 1.97 & 98.5\% \\
4 & 9.45 & 3.79 & 94.8\% \\
8 & 5.11 & 7.01 & 87.6\% \\
16 & 3.02 & 11.87 & 74.2\% \\
32 & 2.25 & 15.93 & 49.8\% \\
64 & 2.05 & 17.48 & 27.3\% \\
128 & 2.01 & 17.83 & 13.9\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:mpi_scaling}
\end{table}

MPI maintains higher efficiency than OpenMP at moderate scales due to better memory locality, but communication overhead becomes significant beyond 32 processes.

\subsubsection{Hybrid Performance}
The hybrid solver demonstrates superior scalability by combining both paradigms:

\begin{table}[htbp]
\caption{Hybrid Scaling with Various Configurations (11×11 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{MPI} & \textbf{OMP} & \textbf{Total} & \textbf{Time} & \textbf{Speedup} \\
\textbf{Procs} & \textbf{Threads} & \textbf{Cores} & \textbf{(s)} & \\
\midrule
1 & 1 & 1 & 142.36 & 1.00 \\
1 & 4 & 4 & 37.82 & 3.76 \\
2 & 2 & 4 & 36.94 & 3.85 \\
4 & 1 & 4 & 38.45 & 3.70 \\
2 & 8 & 16 & 10.28 & 13.85 \\
4 & 4 & 16 & 9.87 & 14.42 \\
8 & 2 & 16 & 10.95 & 13.00 \\
4 & 16 & 64 & 5.12 & 27.80 \\
8 & 8 & 64 & 5.03 & 28.31 \\
16 & 4 & 64 & 5.38 & 26.46 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:hybrid_scaling}
\end{table}

The hybrid approach achieves the best overall speedup (28.3×) by effectively utilizing both parallelization levels. The 8×8 configuration (8 MPI processes with 8 OpenMP threads each) provides optimal performance for 64 cores.

\subsection{Task Generation Factor Analysis}
We investigated the impact of the task generation factor—a multiplier that controls work unit granularity. Figure \ref{fig:task_factor} shows performance variation with different factors:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/task_factor_analysis.png}
\caption{Performance impact of task generation factor for 64 cores. Optimal performance occurs around factor = 4-8.}
\label{fig:task_factor}
\end{figure}

Optimal performance occurs around factor = 4-8, balancing sufficient parallelism against overhead. Lower factors cause load imbalance, while higher factors increase coordination overhead.

\subsection{Weak Scaling Analysis}
To evaluate scalability with increasing problem size, we conducted weak scaling experiments where puzzle size grows proportionally with processor count:

\begin{table}[htbp]
\caption{Weak Scaling Efficiency}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Cores} & \textbf{Puzzle} & \textbf{Time (s)} & \textbf{Efficiency} \\
\midrule
1 & 5×5 & 0.23 & 100.0\% \\
4 & 7×7 & 0.31 & 92.7\% \\
16 & 9×9 & 0.45 & 87.3\% \\
64 & 11×11 & 0.68 & 79.4\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:weak_scaling}
\end{table}

The solver maintains good weak scaling efficiency, demonstrating its ability to handle larger problems with proportionally more resources.

\subsection{Performance Summary}
Figure \ref{fig:speedup_comparison} summarizes the speedup achieved by each implementation:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/speedup_chart.png}
\caption{Speedup comparison across all implementations. The hybrid approach consistently outperforms single-paradigm solutions.}
\label{fig:speedup_comparison}
\end{figure}

Key observations:
\begin{itemize}
    \item \textbf{Sequential Optimization:} Pre-coloring provides 14× speedup, essential for all parallel versions
    \item \textbf{Shared Memory:} OpenMP excels within a single node but plateaus at 32 threads
    \item \textbf{Distributed Memory:} MPI scales better across nodes but has higher communication overhead
    \item \textbf{Hybrid Approach:} Combines strengths of both paradigms, achieving best overall performance
\end{itemize}