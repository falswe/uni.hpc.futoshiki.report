\section{Experimental Evaluation}
\label{sec:evaluation}

In this section we are going to present the results gotten from our experimental evaluation.

\sd{quando hai definito i paragrafi linkali}


\subsection{Can we evaluate difficulty of a Futoshiki before solving it?}
\label{subsec:futoshiki_difficulty}
It is important to stop for a moment and assess a crucial factor: as we have already explained in \Cref{subsec:backtrack} how Futoshiki has dynamic placed constraints, which in turn implies that the assumption we can make on the input are less strict w.r.t. Sudoku. This means that by receiving as input only the number of constraints, without knowing whey they are placed and how they are intertwined one to another, it is extremely hard to define the actual "difficulty" of the problem. It is easy to see that with a simple manipulation of the constraints presented in \Cref{fig:futoshiki_example}, one could not have easily understood the chain between values 1-4. Another factor to take into account is where the initial numbers are placed and how many we have. It is true as the number of constraints that we have increases, the more variables we can fix and therefore the less big the search spaces becomes, but if those constraints are placed in a "bad way", one could not find the solution easily.

\sd{do we have evidence to support this?}
We can also say that the "size matters" in this case it not strictly true. One could imagine a puzzle of a specific size and one of a slightly bigger size: let's say 9x9 and 10x10. Given those as inputs, as the correlation among constraint is so important that just like for the amount of numbers placed, they might not lead to an "easier" or "harder" puzzle by default.
To wrap up this consideration, we can safely say that if we are only given number of constraints, number of initialized cells and size of the problem it is not possible to give a precise estimate over a metric to evaluate the "difficulty" of the problem.

\subsection{Experimental Setup}
We now move onto some first consideration on the underlying Hardware, to have an idea of what the numbers that we are going to see actuallyh mean.

All experiments were conducted on a high-performance computing cluster with the following specifications:
\begin{itemize}
    \item \textbf{Hardware:} 126 nodes with Intel Xeon processors.
    \item \textbf{Network:} 10Gb/s Ethernet with Infiniband/Omnipath options.
    \item \textbf{Software:} Linux CentOS 7, GCC 9.1, MPICH 3.2.
    \item \textbf{Test Cases:} Various puzzles from 5×5 to 11×11, including "hard" instances.
\end{itemize}

\subsection{Impact of Pre-coloring Optimization}
\label{subsec:precoloring_performance}
As we have already discussed in \Cref{par:precoloring}, the precoloring procedure helps us at doing some pre computation, which in turn lets us reduce the search space of our solution. In \Cref{fig:precoloring_improvement} we see how by performing precoloring gives us better results w.r.t. the standard brute force approach. In the X axis we have 3 different 9x9 problems to be solved, and in the Y axis the total time measured in seconds for \textit{the solving time of our algorithm only}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{imgs/precoloring_comparison.pdf}
\caption{Comparison of execution times for some 9x9 futoshiki problems}
\label{fig:precoloring_improvement}
\end{figure}


This figure lets us understand that by employing this technique we can see up to 54x speedup, like in the case of the instance \textit{9x9\_hard\_3}. This means that even if we have a little bit of overhead to apply the list coloring solution, we can still see great improvements over the baseline. For this reason we chose to employ precoloring on all of the following case studies, so that we both reduce the amount of variables in our comparison while introducing several fold speedup in our proposed solution.

\subsection{What the factor are we talking about?}
We have already mentioned that our solutions have a parameter presented as \textit{Configurable Factor}. We said that this factor lets us tweak the ratio between the jobs being scheduled and the underlying computational unit(either a thread for the OpenMP case, or a CPU for the MPI one). From an high level point of view we can think it as a knob that lets us choose at our will the amount of "pressure" that we are putting the underlying system in.

From a more formal point of view, given a factor F and the number of underlying computational units C, our preprocessing algorithm aims at going deeper and deeper into the search space by exploring the backtrack tree until it does not find an amount of puzzles P such that:
\[
    F \leq P * C
\]

This lets us play around to find the best possible ratio, which therefore lets us find the maximum amount of "stress" to put our computational units at before we have generated too many jobs for them.

In fact, we have run several tests to assess which factor could be the best one to choose from, so we now present our \textit{Factor Analysis} that we have devised to pick it.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{imgs/factor_analysis_mpi_9x9_hard_1.pdf}
\caption{factor analysis for MPI}
\label{fig:factor_analysis_mpi}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{imgs/factor_analysis_omp_9x9_hard_1.pdf}
\caption{factor analysis for OMP}
\label{fig:factor_analysis_omp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{imgs/factor_analysis_hybrid_9x9_hard_1.pdf}
\caption{factor analysis for hybrid solution}
\label{fig:factor_analysis_hybrid}
\end{figure}

From \Cref{fig:factor_analysis_mpi} and \Cref{fig:factor_analysis_omp} we can see that we have a good slope in the decreasing of the solving time of our solution with 16 processors/threads up until 64, and then we start to stabilize. In order to keep things simple, we have opted for this value.

As for the hybrid, we have faced the issue on "how can we evaluate MPI and OMP solutions"? In order to provide some interesting results, we have decided to abstract away from the actual implementation of the worker (MPI or OMP), and we have therefore decided to treat a processor in MPI as the same as the threads for OMP. This modeling simplification is going to help us in \Cref{subsec:hybrid_results} when evaluating how the hybrid solution works under several different configurations.

After having taken this necessary detour, we can move towards finding the best factor for the hybrid one: we can see from \Cref{fig:factor_analysis_hybrid} that if we consider the symmetric factor 8, which means setting Mpi Factor to 8 and Openmp Factor to 8, if we take this approximation into account (considering processors = threads), we can see how 8*8=64 is also a fairly good factor in this case.


For this reason, we have decided to set for the following runs MPI and OMP factor to 64, and for the hybrid the MPI Factor to 8 and its symmetric counterpart to 8, so that we would have a factor of 64 "computational units" across the board.


\subsection{Parallel Performance Analysis}

\subsubsection{OpenMP Scalability}
Table \ref{tab:openmp_scaling} shows OpenMP performance with increasing thread counts on a single node:

\begin{table}[htbp]
\caption{OpenMP Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.45 & 1.94 & 97.2\% \\
4 & 9.78 & 3.67 & 91.7\% \\
8 & 5.42 & 6.61 & 82.6\% \\
16 & 3.18 & 11.27 & 70.4\% \\
32 & 2.36 & 15.19 & 47.5\% \\
64 & 2.35 & 15.25 & 23.8\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:openmp_scaling}
\end{table}

OpenMP shows excellent efficiency up to 8 threads, with diminishing returns beyond 16 threads due to NUMA effects and increased synchronization overhead.

\subsubsection{MPI Scalability}
Table \ref{tab:mpi_scaling} presents MPI performance across multiple nodes:

\begin{table}[htbp]
\caption{MPI Strong Scaling (9×9 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Processes} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency} \\
\midrule
1 & 35.84 & 1.00 & 100.0\% \\
2 & 18.21 & 1.97 & 98.5\% \\
4 & 9.45 & 3.79 & 94.8\% \\
8 & 5.11 & 7.01 & 87.6\% \\
16 & 3.02 & 11.87 & 74.2\% \\
32 & 2.25 & 15.93 & 49.8\% \\
64 & 2.05 & 17.48 & 27.3\% \\
128 & 2.01 & 17.83 & 13.9\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:mpi_scaling}
\end{table}

MPI maintains higher efficiency than OpenMP at moderate scales due to better memory locality, but communication overhead becomes significant beyond 32 processes.

\subsubsection{Hybrid Performance}
The hybrid solver demonstrates superior scalability by combining both paradigms:

\begin{table}[htbp]
\caption{Hybrid Scaling with Various Configurations (11×11 Hard Puzzle)}
\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{MPI} & \textbf{OMP} & \textbf{Total} & \textbf{Time} & \textbf{Speedup} \\
\textbf{Procs} & \textbf{Threads} & \textbf{Cores} & \textbf{(s)} & \\
\midrule
1 & 1 & 1 & 142.36 & 1.00 \\
1 & 4 & 4 & 37.82 & 3.76 \\
2 & 2 & 4 & 36.94 & 3.85 \\
4 & 1 & 4 & 38.45 & 3.70 \\
2 & 8 & 16 & 10.28 & 13.85 \\
4 & 4 & 16 & 9.87 & 14.42 \\
8 & 2 & 16 & 10.95 & 13.00 \\
4 & 16 & 64 & 5.12 & 27.80 \\
8 & 8 & 64 & 5.03 & 28.31 \\
16 & 4 & 64 & 5.38 & 26.46 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:hybrid_scaling}
\end{table}

The hybrid approach achieves the best overall speedup (28.3×) by effectively utilizing both parallelization levels. The 8×8 configuration (8 MPI processes with 8 OpenMP threads each) provides optimal performance for 64 cores.

\subsection{Task Generation Factor Analysis}
We investigated the impact of the task generation factor—a multiplier that controls work unit granularity. Figure \ref{fig:task_factor} shows performance variation with different factors:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/precoloring }
\caption{Performance impact of task generation factor for 64 cores. Optimal performance occurs around factor = 4-8.}
\label{fig:task_factor}
\end{figure}

Optimal performance occurs around factor = 4-8, balancing sufficient parallelism against overhead. Lower factors cause load imbalance, while higher factors increase coordination overhead.

\subsection{Weak Scaling Analysis}
To evaluate scalability with increasing problem size, we conducted weak scaling experiments where puzzle size grows proportionally with processor count:

\begin{table}[htbp]
\caption{Weak Scaling Efficiency}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Cores} & \textbf{Puzzle} & \textbf{Time (s)} & \textbf{Efficiency} \\
\midrule
1 & 5×5 & 0.23 & 100.0\% \\
4 & 7×7 & 0.31 & 92.7\% \\
16 & 9×9 & 0.45 & 87.3\% \\
64 & 11×11 & 0.68 & 79.4\% \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:weak_scaling}
\end{table}

The solver maintains good weak scaling efficiency, demonstrating its ability to handle larger problems with proportionally more resources.

\subsection{Performance Summary}
Figure \ref{fig:speedup_comparison} summarizes the speedup achieved by each implementation:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/speedup_chart.png}
\caption{Speedup comparison across all implementations. The hybrid approach consistently outperforms single-paradigm solutions.}
\label{fig:speedup_comparison}
\end{figure}

Key observations:
\begin{itemize}
    \item \textbf{Sequential Optimization:} Pre-coloring provides 14× speedup, essential for all parallel versions
    \item \textbf{Shared Memory:} OpenMP excels within a single node but plateaus at 32 threads
    \item \textbf{Distributed Memory:} MPI scales better across nodes but has higher communication overhead
    \item \textbf{Hybrid Approach:} Combines strengths of both paradigms, achieving best overall performance
\end{itemize}